{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oqvqG4UrCg8V"
      },
      "source": [
        "<a target=\"_blank\" href=\"https://colab.research.google.com/github/cohere-ai/cohere-developer-experience/blob/main/notebooks/guides/getting-started/v2/tutorial_pt6_v2.ipynb\">\n",
        "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
        "</a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KCBjx0ZLCg8X"
      },
      "source": [
        "# RAG"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OxGSKPryCg8X"
      },
      "source": [
        "The Chat endpoint provides comprehensive support for various text generation use cases, including retrieval-augmented generation (RAG).\n",
        "\n",
        "While LLMs are good at maintaining the context of the conversation and generating responses, they can be prone to hallucinate and include factually incorrect or incomplete information in their responses.\n",
        "\n",
        "RAG enables a model to access and utilize supplementary information from external documents, thereby improving the accuracy of its responses.\n",
        "\n",
        "When using RAG with the Chat endpoint, these responses are backed by fine-grained citations linking to the source documents. This makes the responses easily verifiable.\n",
        "\n",
        "In this tutorial, you'll learn about:\n",
        "- Basic RAG\n",
        "- Search query generation\n",
        "- Retrieval with Embed\n",
        "- Reranking with Rerank\n",
        "- Response and citation generation\n",
        "\n",
        "You'll learn these by building an onboarding assistant for new hires."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1sRvHOwqCg8Y"
      },
      "source": [
        "## Setup\n",
        "\n",
        "To get started, first we need to install the `cohere` library and create a Cohere client."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "T49fZYKqCg8Y",
        "outputId": "72ede862-2c7f-4b22-e20c-150ad16d548e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting cohere\n",
            "  Downloading cohere-5.14.0-py3-none-any.whl.metadata (3.4 kB)\n",
            "Collecting fastavro<2.0.0,>=1.9.4 (from cohere)\n",
            "  Downloading fastavro-1.10.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.5 kB)\n",
            "Requirement already satisfied: httpx>=0.21.2 in /usr/local/lib/python3.11/dist-packages (from cohere) (0.28.1)\n",
            "Collecting httpx-sse==0.4.0 (from cohere)\n",
            "  Downloading httpx_sse-0.4.0-py3-none-any.whl.metadata (9.0 kB)\n",
            "Requirement already satisfied: pydantic>=1.9.2 in /usr/local/lib/python3.11/dist-packages (from cohere) (2.10.6)\n",
            "Requirement already satisfied: pydantic-core<3.0.0,>=2.18.2 in /usr/local/lib/python3.11/dist-packages (from cohere) (2.27.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from cohere) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<1,>=0.15 in /usr/local/lib/python3.11/dist-packages (from cohere) (0.21.0)\n",
            "Collecting types-requests<3.0.0,>=2.0.0 (from cohere)\n",
            "  Downloading types_requests-2.32.0.20250306-py3-none-any.whl.metadata (2.3 kB)\n",
            "Requirement already satisfied: typing_extensions>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from cohere) (4.12.2)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx>=0.21.2->cohere) (3.7.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx>=0.21.2->cohere) (2025.1.31)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.21.2->cohere) (1.0.7)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from httpx>=0.21.2->cohere) (3.10)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx>=0.21.2->cohere) (0.14.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=1.9.2->cohere) (0.7.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.0.0->cohere) (3.4.1)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.0.0->cohere) (2.3.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.11/dist-packages (from tokenizers<1,>=0.15->cohere) (0.28.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers<1,>=0.15->cohere) (3.17.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers<1,>=0.15->cohere) (2024.10.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers<1,>=0.15->cohere) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers<1,>=0.15->cohere) (6.0.2)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers<1,>=0.15->cohere) (4.67.1)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx>=0.21.2->cohere) (1.3.1)\n",
            "Downloading cohere-5.14.0-py3-none-any.whl (253 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m253.9/253.9 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading httpx_sse-0.4.0-py3-none-any.whl (7.8 kB)\n",
            "Downloading fastavro-1.10.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m46.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading types_requests-2.32.0.20250306-py3-none-any.whl (20 kB)\n",
            "Installing collected packages: types-requests, httpx-sse, fastavro, cohere\n",
            "Successfully installed cohere-5.14.0 fastavro-1.10.0 httpx-sse-0.4.0 types-requests-2.32.0.20250306\n"
          ]
        }
      ],
      "source": [
        "\n",
        "!pip install cohere\n",
        "\n",
        "import cohere\n",
        "import numpy as np\n",
        "import json\n",
        "from typing import List\n",
        "\n",
        "co = cohere.ClientV2(api_key=\"PZuwRmsDsswNA79CmLB9wv36uNoKKoRUyjhaDSUv\") # Get your free API key: https://dashboard.cohere.com/api-keys"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BhzFEbEkCg8Z"
      },
      "source": [
        "## Basic RAG\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Go4aRO47Cg8Z"
      },
      "source": [
        "To see how RAG works, let's define the documents that the application has access to. We'll use a short list of documents consisting of internal FAQs about the fictitious company Co1t (in production, these documents are massive).\n",
        "\n",
        "In this example, each document is a `data` object with one field, `text`. But we can define any number of fields we want, depending on the nature of the documents. For example, emails could contain `title` and `text` fields."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "B_Ll3gQACg8Z"
      },
      "outputs": [],
      "source": [
        "documents = [\n",
        "  {\n",
        "    \"data\": {\n",
        "      \"text\": \"Reimbursing Travel Expenses: Easily manage your travel expenses by submitting them through our finance tool. Approvals are prompt and straightforward.\"\n",
        "    }\n",
        "  },\n",
        "  {\n",
        "    \"data\": {\n",
        "      \"text\": \"Working from Abroad: Working remotely from another country is possible. Simply coordinate with your manager and ensure your availability during core hours.\"\n",
        "    }\n",
        "  },\n",
        "  {\n",
        "    \"data\": {\n",
        "      \"text\": \"Health and Wellness Benefits: We care about your well-being and offer gym memberships, on-site yoga classes, and comprehensive health insurance.\"\n",
        "    }\n",
        "  }\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JOTwUyhgCg8a"
      },
      "source": [
        "To call the Chat API with RAG, pass the following parameters at a minimum. This tells the model to run in RAG-mode and use these documents in its response.\n",
        "\n",
        "- `model` for the model ID\n",
        "- `messages` for the user's query.\n",
        "- `documents` for defining the documents.\n",
        "\n",
        "Let's create a query asking about the company's support for personal well-being, which is not going to be available to the model based on the data its trained on. It will need to use external documents.\n",
        "\n",
        "RAG introduces additional objects in the Chat response. One of them is `citations`, which contains details about:\n",
        "- specific text spans from the retrieved documents on which the response is grounded.\n",
        "- the documents referenced in the citations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "axoeD3rvCg8a",
        "outputId": "8f4f85ab-d89d-4dba-d13c-26d2dd34c3df"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Yes, there are health benefits. We offer gym memberships, on-site yoga classes, and comprehensive health insurance.\n",
            "\n",
            "CITATIONS:\n",
            "start=41 end=115 text='gym memberships, on-site yoga classes, and comprehensive health insurance.' sources=[DocumentSource(type='document', id='doc:2', document={'id': 'doc:2', 'text': 'Health and Wellness Benefits: We care about your well-being and offer gym memberships, on-site yoga classes, and comprehensive health insurance.'})] \n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Add the user query\n",
        "query = \"Are there health benefits?\"\n",
        "\n",
        "# Generate the response\n",
        "response = co.chat(model=\"command-r-plus-08-2024\",\n",
        "                   messages=[{'role': 'user', 'content': query}],\n",
        "                   documents=documents)\n",
        "\n",
        "# Display the response\n",
        "print(response.message.content[0].text)\n",
        "\n",
        "# Display the citations and source documents\n",
        "if response.message.citations:\n",
        "    print(\"\\nCITATIONS:\")\n",
        "    for citation in response.message.citations:\n",
        "        print(citation, \"\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jo7uezSiCg8b"
      },
      "source": [
        "## Search query generation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yh87OKgLCg8b"
      },
      "source": [
        "The previous example showed how to get started with RAG, and in particular, the augmented generation portion of RAG. But as its name implies, RAG consists of other steps, such as retrieval.\n",
        "\n",
        "In a basic RAG application, the steps involved are:\n",
        "\n",
        "- Transforming the user message into search queries\n",
        "- Retrieving relevant documents for a given search query\n",
        "- Generating the response and citations\n",
        "\n",
        "Let's now look at the first step—search query generation. The chatbot needs to generate an optimal set of search queries to use for retrieval.\n",
        "\n",
        "There are different possible approaches to this. In this example, we'll take a [tool use](v2/docs/tool-use) approach.\n",
        "\n",
        "Here, we build a tool that takes a user query and returns a list of relevant document snippets for that query. The tool can generate zero, one or multiple search queries depending on the user query."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q9f0ihiBCg8b"
      },
      "outputs": [],
      "source": [
        "def generate_search_queries(message: str) -> List[str]:\n",
        "\n",
        "    # Define the query generation tool\n",
        "    query_gen_tool = [\n",
        "        {\n",
        "            \"type\": \"function\",\n",
        "            \"function\": {\n",
        "                \"name\": \"internet_search\",\n",
        "                \"description\": \"Returns a list of relevant document snippets for a textual query retrieved from the internet\",\n",
        "                \"parameters\": {\n",
        "                    \"type\": \"object\",\n",
        "                    \"properties\": {\n",
        "                        \"queries\": {\n",
        "                            \"type\": \"array\",\n",
        "                            \"items\": {\"type\": \"string\"},\n",
        "                            \"description\": \"a list of queries to search the internet with.\",\n",
        "                        }\n",
        "                    },\n",
        "                    \"required\": [\"queries\"],\n",
        "                },\n",
        "            },\n",
        "        }\n",
        "    ]\n",
        "\n",
        "\n",
        "    # Define a preamble to optimize search query generation\n",
        "    instructions = \"Write a search query that will find helpful information for answering the user's question accurately. If you need more than one search query, write a list of search queries. If you decide that a search is very unlikely to find information that would be useful in constructing a response to the user, you should instead directly answer.\"\n",
        "\n",
        "    # Generate search queries (if any)\n",
        "    search_queries = []\n",
        "\n",
        "    res = co.chat(\n",
        "        model=\"command-r-08-2024\",\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": instructions},\n",
        "            {\"role\": \"user\", \"content\": message},\n",
        "        ],\n",
        "        tools=query_gen_tool\n",
        "    )\n",
        "\n",
        "    if res.message.tool_calls:\n",
        "        for tc in res.message.tool_calls:\n",
        "            queries = json.loads(tc.function.arguments)[\"queries\"]\n",
        "            search_queries.extend(queries)\n",
        "\n",
        "    return search_queries"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d03bL7hICg8b"
      },
      "source": [
        "In the example above, the tool breaks down the user message into two separate queries."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VMEdVZQDCg8b",
        "outputId": "ca7a9c1b-e768-40b4-a0a0-27277bcca459"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['how to stay connected with the company', 'do companies organise team events']\n"
          ]
        }
      ],
      "source": [
        "query = \"How to stay connected with the company, and do you organize team events?\"\n",
        "queries_for_search = generate_search_queries(query)\n",
        "print(queries_for_search)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qA0JmpabCg8c"
      },
      "source": [
        "And in the example below, the tool decides that one query is sufficient."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MRfUcBkpCg8c",
        "outputId": "d0afb73b-a7c8-4a91-98f9-ad21b94734b0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['How flexible are the working hours?']\n"
          ]
        }
      ],
      "source": [
        "query = \"How flexible are the working hours\"\n",
        "queries_for_search = generate_search_queries(query)\n",
        "print(queries_for_search)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gtaNJCqvCg8c"
      },
      "source": [
        "And in the example below, the tool decides that no retrieval is needed to answer the query."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AyIwt-V-Cg8c",
        "outputId": "f0d4dd83-f979-47c9-b771-24da81282578"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[]\n"
          ]
        }
      ],
      "source": [
        "query = \"What is 2 + 2\"\n",
        "queries_for_search = generate_search_queries(query)\n",
        "print(queries_for_search)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ReBNo3pECg8c"
      },
      "source": [
        "## Retrieval with Embed"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2_P3unsICg8c"
      },
      "source": [
        "Given the search query, we need a way to retrieve the most relevant documents from a large collection of documents.\n",
        "\n",
        "This is where we can leverage text embeddings through the Embed endpoint. It enables semantic search, which lets us to compare the semantic meaning of the documents and the query. It solves the problem faced by the more traditional approach of lexical search, which is great at finding keyword matches, but struggles at capturing the context or meaning of a piece of text.\n",
        "\n",
        "The Embed endpoint takes in texts as input and returns embeddings as output.\n",
        "\n",
        "First, we need to embed the documents to search from. We call the Embed endpoint using `co.embed()` and pass the following arguments:\n",
        "\n",
        "- `model`: Here we choose `embed-english-v3.0`, which generates embeddings of size 1024\n",
        "- `input_type`: We choose `search_document` to ensure the model treats these as the documents (instead of the query) for search\n",
        "- `texts`: The list of texts (the FAQs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8nMCKc6XCg8d"
      },
      "outputs": [],
      "source": [
        "# Define the documents\n",
        "faqs_long = [\n",
        "    {\n",
        "        \"data\": {\n",
        "            \"text\": \"Joining Slack Channels: You will receive an invite via email. Be sure to join relevant channels to stay informed and engaged.\"\n",
        "        }\n",
        "    },\n",
        "    {\n",
        "        \"data\": {\n",
        "            \"text\": \"Finding Coffee Spots: For your caffeine fix, head to the break room's coffee machine or cross the street to the café for artisan coffee.\"\n",
        "        }\n",
        "    },\n",
        "    {\n",
        "        \"data\": {\n",
        "            \"text\": \"Team-Building Activities: We foster team spirit with monthly outings and weekly game nights. Feel free to suggest new activity ideas anytime!\"\n",
        "        }\n",
        "    },\n",
        "    {\n",
        "        \"data\": {\n",
        "            \"text\": \"Working Hours Flexibility: We prioritize work-life balance. While our core hours are 9 AM to 5 PM, we offer flexibility to adjust as needed.\"\n",
        "        }\n",
        "    },\n",
        "    {\n",
        "        \"data\": {\n",
        "            \"text\": \"Side Projects Policy: We encourage you to pursue your passions. Just be mindful of any potential conflicts of interest with our business.\"\n",
        "        }\n",
        "    },\n",
        "    {\n",
        "        \"data\": {\n",
        "            \"text\": \"Reimbursing Travel Expenses: Easily manage your travel expenses by submitting them through our finance tool. Approvals are prompt and straightforward.\"\n",
        "        }\n",
        "    },\n",
        "    {\n",
        "        \"data\": {\n",
        "            \"text\": \"Working from Abroad: Working remotely from another country is possible. Simply coordinate with your manager and ensure your availability during core hours.\"\n",
        "        }\n",
        "    },\n",
        "    {\n",
        "        \"data\": {\n",
        "            \"text\": \"Health and Wellness Benefits: We care about your well-being and offer gym memberships, on-site yoga classes, and comprehensive health insurance.\"\n",
        "        }\n",
        "    },\n",
        "    {\n",
        "        \"data\": {\n",
        "            \"text\": \"Performance Reviews Frequency: We conduct informal check-ins every quarter and formal performance reviews twice a year.\"\n",
        "        }\n",
        "    },\n",
        "    {\n",
        "        \"data\": {\n",
        "            \"text\": \"Proposing New Ideas: Innovation is welcomed! Share your brilliant ideas at our weekly team meetings or directly with your team lead.\"\n",
        "        }\n",
        "    },\n",
        "]\n",
        "\n",
        "# Embed the documents\n",
        "doc_emb = co.embed(\n",
        "            model=\"embed-english-v3.0\",\n",
        "            input_type=\"search_document\",\n",
        "            texts=[doc['data']['text'] for doc in faqs_long],\n",
        "            embedding_types=[\"float\"]).embeddings.float"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0KRv1st3Cg8d"
      },
      "source": [
        "Next, we add a query, which asks about how to get to know the team.\n",
        "\n",
        "We choose `search_query` as the `input_type` to ensure the model treats this as the query (instead of the documents) for search."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J4UAij3bCg8d",
        "outputId": "5ec217b4-81e3-4dfc-9450-a5688f1972e5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Search query:  how to get to know your teammates\n"
          ]
        }
      ],
      "source": [
        "# Add the user query\n",
        "query = \"How to get to know my teammates\"\n",
        "\n",
        "# Generate the search query\n",
        "# Note: For simplicity, we are assuming only one query generated. For actual implementations, you will need to perform search for each query.\n",
        "queries_for_search = generate_search_queries(query)[0]\n",
        "print(\"Search query: \", queries_for_search)\n",
        "\n",
        "# Embed the search query\n",
        "query_emb = co.embed(\n",
        "    model=\"embed-english-v3.0\",\n",
        "    input_type=\"search_query\",\n",
        "    texts=[queries_for_search],\n",
        "    embedding_types=[\"float\"]).embeddings.float"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VL_ch0IlCg8d"
      },
      "source": [
        "Now, we want to search for the most relevant documents to the query. For this, we make use of the `numpy` library to compute the similarity between each query-document pair using the dot product approach.\n",
        "\n",
        "Each query-document pair returns a score, which represents how similar the pair are. We then sort these scores in descending order and select the top most similar pairs, which we choose 5 (this is an arbitrary choice, you can choose any number).\n",
        "\n",
        "Here, we show the most relevant documents with their similarity scores."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3F_tO0lGCg8d",
        "outputId": "5637bbdb-05d9-4dab-96bc-4ad4aa0cefa2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Rank: 1\n",
            "Score: 0.32653470360872655\n",
            "Document: {'data': {'text': 'Team-Building Activities: We foster team spirit with monthly outings and weekly game nights. Feel free to suggest new activity ideas anytime!'}}\n",
            "\n",
            "Rank: 2\n",
            "Score: 0.26851855352264786\n",
            "Document: {'data': {'text': 'Proposing New Ideas: Innovation is welcomed! Share your brilliant ideas at our weekly team meetings or directly with your team lead.'}}\n",
            "\n",
            "Rank: 3\n",
            "Score: 0.2581341975304149\n",
            "Document: {'data': {'text': 'Joining Slack Channels: You will receive an invite via email. Be sure to join relevant channels to stay informed and engaged.'}}\n",
            "\n",
            "Rank: 4\n",
            "Score: 0.18633336738178463\n",
            "Document: {'data': {'text': \"Finding Coffee Spots: For your caffeine fix, head to the break room's coffee machine or cross the street to the café for artisan coffee.\"}}\n",
            "\n",
            "Rank: 5\n",
            "Score: 0.13022396595682814\n",
            "Document: {'data': {'text': 'Health and Wellness Benefits: We care about your well-being and offer gym memberships, on-site yoga classes, and comprehensive health insurance.'}}\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Compute dot product similarity and display results\n",
        "n = 5\n",
        "scores = np.dot(query_emb, np.transpose(doc_emb))[0]\n",
        "max_idx = np.argsort(-scores)[:n]\n",
        "\n",
        "retrieved_documents = [faqs_long[item] for item in max_idx]\n",
        "\n",
        "for rank, idx in enumerate(max_idx):\n",
        "    print(f\"Rank: {rank+1}\")\n",
        "    print(f\"Score: {scores[idx]}\")\n",
        "    print(f\"Document: {retrieved_documents[rank]}\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0qitGAsyCg8d"
      },
      "source": [
        "Reranking can boost the results from semantic or lexical search further. The Rerank endpoint takes a list of search results and reranks them according to the most relevant documents to a query. This requires just a single line of code to implement.\n",
        "\n",
        "We call the endpoint using `co.rerank()` and pass the following arguments:\n",
        "\n",
        "- `query`: The user query\n",
        "- `documents`: The list of documents we get from the semantic search results\n",
        "- `top_n`: The top reranked documents to select\n",
        "- `model`: We choose Rerank English 3\n",
        "\n",
        "Looking at the results, we see that since the query is about getting to know the team, the document that talks about joining Slack channels is now ranked higher (1st) compared to earlier (3rd).\n",
        "\n",
        "Here we select `top_n` to be 2, which will be the documents we will pass next for response generation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FxvX8ZoZCg8e",
        "outputId": "83ad9243-e519-4116-9ded-e0ca737ed792"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Rank: 1\n",
            "Score: 0.0040072887\n",
            "Document: {'data': {'text': 'Joining Slack Channels: You will receive an invite via email. Be sure to join relevant channels to stay informed and engaged.'}}\n",
            "\n",
            "Rank: 2\n",
            "Score: 0.0020829707\n",
            "Document: {'data': {'text': 'Team-Building Activities: We foster team spirit with monthly outings and weekly game nights. Feel free to suggest new activity ideas anytime!'}}\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Rerank the documents\n",
        "results = co.rerank(query=queries_for_search,\n",
        "                    documents=[doc['data']['text'] for doc in retrieved_documents],\n",
        "                    top_n=2,\n",
        "                    model='rerank-english-v3.0')\n",
        "\n",
        "# Display the reranking results\n",
        "for idx, result in enumerate(results.results):\n",
        "    print(f\"Rank: {idx+1}\")\n",
        "    print(f\"Score: {result.relevance_score}\")\n",
        "    print(f\"Document: {retrieved_documents[result.index]}\\n\")\n",
        "\n",
        "reranked_documents = [retrieved_documents[result.index] for result in results.results]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "novP7CvlCg8e"
      },
      "source": [
        "Finally we reach the step that we saw in the earlier `Basic RAG` section.\n",
        "\n",
        "To call the Chat API with RAG, we pass the following parameters. This tells the model to run in RAG-mode and use these documents in its response.\n",
        "\n",
        "- `model` for the model ID\n",
        "- `messages` for the user's query.\n",
        "- `documents` for defining the documents.\n",
        "\n",
        "The response is then generated based on the the query and the documents retrieved.\n",
        "\n",
        "RAG introduces additional objects in the Chat response. One of them is `citations`, which contains details about:\n",
        "- specific text spans from the retrieved documents on which the response is grounded.\n",
        "- the documents referenced in the citations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bLkO3G_BCg8e",
        "outputId": "1730a309-a241-4e7d-98d5-ef4df76e02d1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "You can get to know your teammates by joining Slack channels and participating in team-building activities. You will receive an invite via email to join relevant channels to stay informed and engaged. There are monthly outings and weekly game nights to foster team spirit. You can suggest new activity ideas at any time.\n",
            "\n",
            "CITATIONS:\n",
            "start=38 end=60 text='joining Slack channels' sources=[DocumentSource(type='document', id='doc:0', document={'id': 'doc:0', 'text': 'Joining Slack Channels: You will receive an invite via email. Be sure to join relevant channels to stay informed and engaged.'})] \n",
            "\n",
            "start=82 end=107 text='team-building activities.' sources=[DocumentSource(type='document', id='doc:1', document={'id': 'doc:1', 'text': 'Team-Building Activities: We foster team spirit with monthly outings and weekly game nights. Feel free to suggest new activity ideas anytime!'})] \n",
            "\n",
            "start=117 end=144 text='receive an invite via email' sources=[DocumentSource(type='document', id='doc:0', document={'id': 'doc:0', 'text': 'Joining Slack Channels: You will receive an invite via email. Be sure to join relevant channels to stay informed and engaged.'})] \n",
            "\n",
            "start=148 end=170 text='join relevant channels' sources=[DocumentSource(type='document', id='doc:0', document={'id': 'doc:0', 'text': 'Joining Slack Channels: You will receive an invite via email. Be sure to join relevant channels to stay informed and engaged.'})] \n",
            "\n",
            "start=174 end=200 text='stay informed and engaged.' sources=[DocumentSource(type='document', id='doc:0', document={'id': 'doc:0', 'text': 'Joining Slack Channels: You will receive an invite via email. Be sure to join relevant channels to stay informed and engaged.'})] \n",
            "\n",
            "start=211 end=249 text='monthly outings and weekly game nights' sources=[DocumentSource(type='document', id='doc:1', document={'id': 'doc:1', 'text': 'Team-Building Activities: We foster team spirit with monthly outings and weekly game nights. Feel free to suggest new activity ideas anytime!'})] \n",
            "\n",
            "start=253 end=272 text='foster team spirit.' sources=[DocumentSource(type='document', id='doc:1', document={'id': 'doc:1', 'text': 'Team-Building Activities: We foster team spirit with monthly outings and weekly game nights. Feel free to suggest new activity ideas anytime!'})] \n",
            "\n",
            "start=281 end=320 text='suggest new activity ideas at any time.' sources=[DocumentSource(type='document', id='doc:1', document={'id': 'doc:1', 'text': 'Team-Building Activities: We foster team spirit with monthly outings and weekly game nights. Feel free to suggest new activity ideas anytime!'})] \n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Generate the response\n",
        "response = co.chat(model=\"command-r-plus-08-2024\",\n",
        "                   messages=[{'role': 'user', 'content': query}],\n",
        "                   documents=reranked_documents)\n",
        "\n",
        "# Display the response\n",
        "print(response.message.content[0].text)\n",
        "\n",
        "# Display the citations and source documents\n",
        "if response.message.citations:\n",
        "    print(\"\\nCITATIONS:\")\n",
        "    for citation in response.message.citations:\n",
        "        print(citation, \"\\n\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}